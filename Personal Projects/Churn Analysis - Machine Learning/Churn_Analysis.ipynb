{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujbpu8QfkbaV"
      },
      "source": [
        "### IMPORTS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_SkK1nXkWuo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from typing import Union, Tuple, List, Any\n",
        "import operator\n",
        "%pip install sweetviz\n",
        "%pip install imblearn\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import OrderedDict\n",
        "import sweetviz as sv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9Bk6hveA6yb"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akT4Z3-5oAlj"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIJJa8qakrqY"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path: Path, return_Xy: bool = True) -> Union[Tuple[np.array, np.array], pd.DataFrame]:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    #Todo : Divide into 2 functions\n",
        "\n",
        "    #df = pd.get_dummies(df, columns=['orbiting_body'])\n",
        "    df = df.drop(columns=['customerID'])\n",
        "    df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)\n",
        "    #['TotalCharges'] = df['TotalCharges'].fillna(df.TotalCharges.median())\n",
        "    df['TotalCharges']=df['TotalCharges'].astype(float)\n",
        "    df['gender'] = df['gender'].map({'Male': 0, 'Female': 1})\n",
        "    df['Partner'] = df['Partner'].map({'No': 0, 'Yes': 1})\n",
        "    df['Dependents'] = df['Dependents'].map({'No': 0, 'Yes': 1})\n",
        "    df['PhoneService'] = df['PhoneService'].map({'No': 0, 'Yes': 1})\n",
        "    df['PaperlessBilling'] = df['PaperlessBilling'].map({'No': 0, 'Yes': 1})\n",
        "\n",
        "    X = df.drop(columns=['Churn'],axis=1)\n",
        "    y = df['Churn']\n",
        "    if return_Xy:\n",
        "        return X, y\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZpsmww8_HJX"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehRTvwXEDDI1",
        "outputId": "49c10902-e57d-4784-b572-92e9bc9de02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FClXzUjb_JrZ"
      },
      "outputs": [],
      "source": [
        "#TO DO: MOUNT DRIVE\n",
        "_path = \"/content/drive/MyDrive/AprendizadoDeMaquina/T2/ibm.csv\"\n",
        "#df=pd.read_csv(_path)\n",
        "X,y=load_dataset(Path(_path))\n",
        "df=load_dataset(_path,False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrK9SeJu_ewI"
      },
      "outputs": [],
      "source": [
        "# check data\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmN2N7y5WGcv"
      },
      "outputs": [],
      "source": [
        "# check types\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9m2PHDvWIB2"
      },
      "outputs": [],
      "source": [
        "# check null content\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU0uCsL0WKId"
      },
      "outputs": [],
      "source": [
        "# check more detailed info\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZFgFQ76WMTO"
      },
      "outputs": [],
      "source": [
        "# check if target needs pre-processing\n",
        "print(df.Churn.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGpRREijWRZa"
      },
      "outputs": [],
      "source": [
        "# pairplot\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jundtuq1WSCQ"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.heatmap(corr,\n",
        "            annot=True,\n",
        "            cmap=\"Blues\"\n",
        "            )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdKPdt2qWY4r"
      },
      "outputs": [],
      "source": [
        "# generate automated report\n",
        "report = sv.analyze(df)\n",
        "# display the report\n",
        "report.show_html('Advertising.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmVSYw84ohMK"
      },
      "source": [
        "### PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FReBwNvZot5R"
      },
      "outputs": [],
      "source": [
        "def evaluate_data(X_train, y_train, X_test, y_test, run: int, rus: bool = True):\n",
        "    metadata = {}\n",
        "    #copy just to make sure that data don't leak\n",
        "    y_test_ = y_test.copy()\n",
        "    X_train_ = X_train.copy()\n",
        "    y_train_ = y_train.copy()\n",
        "    X_test_ = X_test.copy()\n",
        "\n",
        "    # define estimator\n",
        "    estimators = (\n",
        "        SVC(random_state=0), XGBClassifier(random_state=0, scale_pos_weight=3, eval_metric='logloss'), GaussianNB())\n",
        "    \n",
        "    #parameter grid for hyperparameter tuning\n",
        "    param_grid = [[\n",
        "\n",
        "        {\n",
        "            # SVM\n",
        "            'clf__kernel': ['rbf', 'linear'],\n",
        "            'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'clf__gamma': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "        }],\n",
        "\n",
        "        [\n",
        "            # XGBOOST\n",
        "            {\"clf__learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
        "             \"clf__max_depth\": [3, 4, 5, 7, 9, 11, 13, 15],\n",
        "             \"clf__min_child_weight\": [1, 3, 5, 7],\n",
        "             \"clf__gamma\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
        "             \"clf__colsample_bytree\": [0.3, 0.4, 0.5, 0.7],\n",
        "             \"clf__n_estimators\": np.arange(100, 300, 25),\n",
        "             }],\n",
        "        [{\n",
        "            # NAIVE BAYES\n",
        "            'clf__var_smoothing': [1e-9]\n",
        "        }]\n",
        "    ]\n",
        "\n",
        "    if run not in metadata:\n",
        "        metadata[run] = {}\n",
        "    for i, clf in enumerate(estimators):\n",
        "        if type(clf).__name__ not in metadata:\n",
        "            metadata[run][type(clf).__name__] = {}\n",
        "\n",
        "        #pipeline for preprocessing\n",
        "\n",
        "        #get numeric features\n",
        "        numeric_features = [\"MonthlyCharges\", \"TotalCharges\", \"tenure\", \"gender\", \"Partner\", \"Dependents\",\n",
        "                            \"PhoneService\", \"PaperlessBilling\"]\n",
        "        #pipeline for numeric features with KNNImputer and MinMaxScaler\n",
        "        numeric_transformer = Pipeline(\n",
        "            steps=[(\"knnImp\", KNNImputer(n_neighbors=3)), (\"scaler\", MinMaxScaler())]\n",
        "        )\n",
        "\n",
        "\n",
        "        #get categorical features\n",
        "        categorical_features = [\"InternetService\", \"PaymentMethod\", 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n",
        "                                'DeviceProtection', 'TechSupport', 'StreamingMovies', 'Contract']\n",
        "        #pipeline for categorical features with OneHotEncoder\n",
        "        categorical_transformer = Pipeline(\n",
        "            steps=[(\"OneHot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        "        )\n",
        "\n",
        "        # Use columnTransformer to join the two pipelines\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\", numeric_transformer, numeric_features),\n",
        "                (\"cat\", categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "        #main pipeline with RandomUnderSampler, preprocessing, estimator\n",
        "        if rus:\n",
        "            pipe = Pipeline(\n",
        "                steps=[(\"rus\", RandomUnderSampler(random_state=0)), (\"pp\", preprocessor), (\"clf\", clf)]\n",
        "            )\n",
        "        else:\n",
        "           #main pipeline with preprocessing, estimator\n",
        "            pipe = Pipeline(\n",
        "                steps=[(\"pp\", preprocessor), (\"clf\", clf)]\n",
        "            )\n",
        "\n",
        "        #Hyperparameter Tuning\n",
        "        search = RandomizedSearchCV(pipe, param_grid[i], verbose=True, n_iter=20, refit=True, random_state=0)\n",
        "\n",
        "        #fit data\n",
        "        search.fit(X_train_, y_train_)\n",
        "        #predict the X_test\n",
        "        y_pred = search.predict(X_test_)\n",
        "\n",
        "        y_pred_ = y_pred.copy()\n",
        "\n",
        "        y_test_ = np.array(y_test_)\n",
        "        #make logs\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1measure']:\n",
        "            if metric not in metadata[run][type(clf).__name__]:\n",
        "                metadata[run][type(clf).__name__][metric] = {}\n",
        "        metadata[run][type(clf).__name__]['accuracy'] = accuracy(y_test_, y_pred_)\n",
        "        metadata[run][type(clf).__name__]['precision'] = precision(y_test_, y_pred_)\n",
        "        metadata[run][type(clf).__name__]['recall'] = recall(y_test_, y_pred_)\n",
        "        metadata[run][type(clf).__name__]['f1measure'] = f1Measure(y_test_, y_pred_)\n",
        "    return metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z8muA7PgxvJ"
      },
      "source": [
        "# K-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLjXApadixku"
      },
      "outputs": [],
      "source": [
        "class Instance:\n",
        "  def __init__(self, instance_data: Any, label: str):\n",
        "    self._instance_data = instance_data\n",
        "    self._label = label\n",
        "  def data(self):\n",
        "    return self._instance_data\n",
        "  def label(self):\n",
        "    return self._label\n",
        "  def __repr__(self):\n",
        "    return f\"{self._instance_data}-{self._label}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAV6i7Ddebyw"
      },
      "outputs": [],
      "source": [
        "def remainders_merge_policy(k_folds, remainders):\n",
        "    _k_folds = k_folds.copy()\n",
        "    for i, r in enumerate(remainders):\n",
        "        _k_folds[i % len(_k_folds)].append(r)\n",
        "    return _k_folds\n",
        "\n",
        "# non stratified\n",
        "def random_k_folds(instance_list: List[Instance], k: int, merge_remainders=True):\n",
        "    # generate random k folds non stratified\n",
        "    instances = instance_list.copy()\n",
        "    random.Random(0).shuffle(instances)\n",
        "    max_fold_size = len(instances) // k\n",
        "    k_folds = [instances[i * max_fold_size:(i + 1) * max_fold_size] for i in range(k)]\n",
        "    remainders = instances[k * max_fold_size:]\n",
        "    return remainders_merge_policy(k_folds, remainders) if merge_remainders else (k_folds, remainders)\n",
        "\n",
        "\n",
        "def stratified_k_folds(instance_list: List[Instance], k: int, merge_remainders=True):\n",
        "    # generate random k folds stratified\n",
        "    instances = instance_list.copy()\n",
        "    labels = sorted(set([i.label() for i in instances]))\n",
        "    labeled_instances = OrderedDict({l: [i for i in instances if i.label() == l] for l in labels})\n",
        "    label_folds = [random_k_folds(labeled_instances[l], k, merge_remainders=False) for l in labels]\n",
        "    labeled_k_folds = [[e for l in f for e in l] for f, r in label_folds]\n",
        "    k_folds = [[e for l in labeled_k_folds for e in l[i * len(l) // k:(i + 1) * len(l) // k]] for i in range(k)]\n",
        "    k_folds = [sorted(k, key=operator.attrgetter('_instance_data')) for k in k_folds]\n",
        "    remainders = [e for f, r in label_folds for e in r]\n",
        "    return remainders_merge_policy(k_folds, remainders) if merge_remainders else (k_folds, remainders)\n",
        "\n",
        "k=3\n",
        "n_classes=3\n",
        "l = [Instance(i, f\"{chr(ord('A')+i%n_classes)}\") for i in range(11)]\n",
        "print(f\"mock instances: {l}\")\n",
        "print(f\"random_k_folds: {random_k_folds(l, k)}\")\n",
        "print(f\"stratified_k_folds: {stratified_k_folds(l, k)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5jPLehkBTb2"
      },
      "outputs": [],
      "source": [
        "def return_dfs(X, y, k):\n",
        "    listt = []\n",
        "\n",
        "    for i, yy in y.items():\n",
        "        '''\n",
        "        cria uma instancia da classe para cada instancia do dataset\n",
        "        cada instancia possui o index no dataset e a label correspondente\n",
        "        '''\n",
        "        listt.append(Instance(i, yy))\n",
        "    s_folds = stratified_k_folds(listt, k)\n",
        "    print(s_folds)\n",
        "\n",
        "    folds = []\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(k):\n",
        "        '''\n",
        "        para cada i em k(quantidade de folds) pega o indices que estao no fold\n",
        "        e utiliza esses indices para montar os fold de dados e de labels\n",
        "        e em seguida adiciona esse fold criado na lista de todos folds\n",
        "        '''\n",
        "        indx = [a._instance_data for a in s_folds[i]]\n",
        "\n",
        "        folds.append(indx)\n",
        "        data.append(X.iloc[indx])\n",
        "        labels.append(y.iloc[indx])\n",
        "\n",
        "    return data, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUt-XWqr8OFP"
      },
      "outputs": [],
      "source": [
        "def k_fold_CV(X, y, k, rus: bool = True):\n",
        "    data, labels = return_dfs(X, y, k)  #get list of folds of data and labels\n",
        "    all_metadata = {} \n",
        "    for i, (X_test, y_test) in enumerate(zip(data, labels)): \n",
        "        '''\n",
        "             For each fold in the list of folds test the fold with train being\n",
        "             all the list minus the current fold.\n",
        "             The same for the labels.\n",
        "\n",
        "        '''\n",
        "        indx = list(range(0, k, 1))\n",
        "        indx.remove(i)\n",
        "        comp_d = [element for i, element in enumerate(data) if i in indx]\n",
        "        X_train = pd.concat(comp_d)\n",
        "        comp_l = [element for i, element in enumerate(labels) if i in indx]\n",
        "        y_train = pd.concat(comp_l)\n",
        "\n",
        "        metadata = evaluate_data(X_train, y_train, X_test, y_test, i, rus)\n",
        "        all_metadata.update(metadata)\n",
        "    print(all_metadata)\n",
        "    return all_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TESTS"
      ],
      "metadata": {
        "id": "k25gvwIam-HM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1yPhGRO8T04"
      },
      "outputs": [],
      "source": [
        "#test with k=10 and undersample\n",
        "data = k_fold_CV(X, y, 10, True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test with k=10 and without undersample\n",
        "data_w = k_fold_CV(X, y, 10, False)"
      ],
      "metadata": {
        "id": "aTcQtabTkzmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PLOTS"
      ],
      "metadata": {
        "id": "5fi807ywm7q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values(metadata, metric):\n",
        "  # get values from metadata\n",
        "    svc = []\n",
        "    xgb = []\n",
        "    nb = []\n",
        "    for key, v in metadata.items():\n",
        "        for key2, v2 in v.items():\n",
        "            for key3, v3 in v2.items():\n",
        "                if key2 == 'SVC' and key3 == metric:\n",
        "                    svc.append(v3)\n",
        "                if key2 == 'XGBClassifier' and key3 == metric:\n",
        "                    xgb.append(v3)\n",
        "                if key2 == 'GaussianNB' and key3 == metric:\n",
        "                    nb.append(v3)\n",
        "    return svc, xgb, nb\n",
        "\n",
        "\n",
        "def plot_box_metric(metadata, metric):\n",
        "    # boxplots\n",
        "    svc, xgb, nb = get_values(metadata, metric)\n",
        "    names = [['SVC'] * len(svc), ['XGBClassifier'] * len(xgb), ['GaussianNB'] * len(nb)]\n",
        "    dfp = pd.DataFrame(svc + xgb + nb, columns=['Value'])\n",
        "    dfp['Estimator'] = ['SVC'] * len(svc) + ['XGBClassifier'] * len(xgb) + ['GaussianNB'] * len(nb)\n",
        "    ''' \n",
        "    print(dfp.loc[dfp['Estimator'] == 'SVC'].mean(numeric_only=True))\n",
        "    print(dfp.loc[dfp['Estimator'] == 'SVC'].std(numeric_only=True))\n",
        "    print(dfp.loc[dfp['Estimator'] == 'XGBClassifier'].mean(numeric_only=True))\n",
        "    print(dfp.loc[dfp['Estimator'] == 'XGBClassifier'].std(numeric_only=True))\n",
        "    print(dfp.loc[dfp['Estimator'] == 'GaussianNB'].mean(numeric_only=True))\n",
        "    print(dfp.loc[dfp['Estimator'] == 'GaussianNB'].std(numeric_only=True))\n",
        "    '''\n",
        "    sns.boxplot(x='Estimator', y='Value', data=dfp)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XzSXlJn4U2MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot boxplot with undersample\n",
        "print(\"accuracy\")\n",
        "plot_box_metric(data, 'accuracy')\n",
        "print(\"recall\")\n",
        "plot_box_metric(data, 'recall')\n",
        "print(\"precision\")\n",
        "plot_box_metric(data, 'precision')\n",
        "print(\"f1-measure\")\n",
        "plot_box_metric(data, 'f1measure')"
      ],
      "metadata": {
        "id": "lhUlKfj6k-WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot boxplot without undersample\n",
        "print(\"accuracy_no_undersample\")\n",
        "plot_box_metric(data_w, 'accuracy')\n",
        "print(\"recall_no_undersample\")\n",
        "plot_box_metric(data_w, 'recall')\n",
        "print(\"precision_no_undersample\")\n",
        "plot_box_metric(data_w, 'precision')\n",
        "print(\"f1-measure_no_undersample\")\n",
        "plot_box_metric(data_w, 'f1measure')"
      ],
      "metadata": {
        "id": "fwxbwfiZlJ8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF-V5DmUFEDG"
      },
      "source": [
        "### Métricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsE_nXdOFHAD"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_hand(y_test: np.array, y_pred: np.array):\n",
        "    ''' \n",
        "    recebe o y_true e o y_pred e retorna quantos Verdadeiros positivos,\n",
        "    verdadeiros negativos, falsos positivos,falsos negativos ocorreram\n",
        "    '''\n",
        "    VP, VN, FP, FN = 0, 0, 0, 0\n",
        "    for y_true, y_p in zip(y_test, y_pred):\n",
        "        if y_true == 'Yes' and y_p == 'Yes':\n",
        "            VP += 1\n",
        "        elif y_true == 'Yes' and y_p =='No':\n",
        "            FN += 1\n",
        "        elif y_true == 'No' and y_p == 'No':\n",
        "            VN += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "    return VP, VN, FP, FN\n",
        "\n",
        "\n",
        "def accuracy(y_test: np.array, y_pred: np.array):\n",
        "    #calculo acurácia conforme slides de aula\n",
        "    VP, VN, FP, FN = confusion_matrix_hand(y_test, y_pred)\n",
        "    return (VP + VN) / y_test.shape[0]\n",
        "\n",
        "\n",
        "def precision(y_test: np.array, y_pred: np.array):\n",
        "    #calculo precisao conforme slides de aula\n",
        "    VP, VN, FP, FN = confusion_matrix_hand(y_test, y_pred)\n",
        "    if (VP + FP)==0:\n",
        "        return 0\n",
        "    return VP / (VP + FP)\n",
        "\n",
        "\n",
        "def recall(y_test: np.array, y_pred: np.array):\n",
        "    #calculo recall conforme slides de aula\n",
        "    VP, VN, FP, FN = confusion_matrix_hand(y_test, y_pred)\n",
        "    return VP / (VP + FN)\n",
        "\n",
        "\n",
        "def f1Measure(y_test: np.array, y_pred: np.array):\n",
        "    #calculo f1-score conforme slides de aula\n",
        "    if((precision(y_test, y_pred) + recall(y_test, y_pred)))==0:\n",
        "        return 0\n",
        "    return 2 * ((precision(y_test, y_pred) * recall(y_test, y_pred)) /\n",
        "                (precision(y_test, y_pred) + recall(y_test, y_pred)))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}